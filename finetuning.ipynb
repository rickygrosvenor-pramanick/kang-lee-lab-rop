{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef945ab-1c39-439e-9ba8-382b3a667d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, files\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "patients = [\"307\", \"323\", \"324\", \"328\", \"335\", \"345\", \"347\", \"348\", \"350\", \"352\", \"369\", \"379\", \"387\", \"390\", \"402\", \"412\", \n",
    "            \"413\", \"419\", \"428\", \"431\", \"432\", \"437\", \"461\", \"463\", \"468\", \"480\", \"485\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583bd162-87bd-4851-99e4-07a77a2a07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Truncate the CSV Transcripts\n",
    "class OpenAILLM():\n",
    "    def init(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def generate(self, prompt: str):\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a expert therapist and know extensive knowledge on the PHQ-8 diagnostic and how to evaluate patients health given their transcripts.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "def get_summarized_transcript(transcript: str, llm) -> str:\n",
    "    summary_prompt = f\"\"\"\n",
    "    Rewrite the conversation to be shorter. Make sure it is in the same general format as the original conversation. Remove any unnecessary details and keep the main points. Be concise\n",
    "    {transcript}\n",
    "    \"\"\"\n",
    "    summarized_transcript = llm.generate(summary_prompt)\n",
    "    return summarized_transcript\n",
    "\n",
    "def process_transcript(input_csv_path: str, output_csv_path: str, llm) -> None:\n",
    "    \"\"\"\n",
    "    Reads a transcript from a CSV file, summarizes it, and writes the summary to an output CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv_path (str): Path to the input CSV file containing the transcript.\n",
    "    - output_csv_path (str): Path to the output CSV file for the summarized transcript.\n",
    "    - llm: An object representing the language model with a generate method.\n",
    "    \"\"\"\n",
    "    # Read the transcript from the CSV file\n",
    "    transcript = \"\"\n",
    "    with open(input_csv_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile, delimiter='\\t')  # Specify tab as the delimiter\n",
    "        columns = reader.fieldnames\n",
    "        print(f\"Column names in CSV: {columns}\")\n",
    "        for row in reader:\n",
    "            # Join columns to maintain original formatting in each line\n",
    "            transcript += f\"{row['start_time']} {row['stop_time']} {row['speaker']}: {row['value']}\\n\"\n",
    "\n",
    "    # Summarize the transcript\n",
    "    summarized_transcript = get_summarized_transcript(transcript, llm)\n",
    "\n",
    "    # Write the summarized transcript to the output CSV file with the same column format\n",
    "    with open(output_csv_path, mode='w', encoding='utf-8', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['start_time', 'stop_time', 'speaker', 'value'])  # Write header\n",
    "        \n",
    "        # Split summarized transcript by lines and parse each line back to columns\n",
    "        for line in summarized_transcript.strip().split('\\n'):\n",
    "            # Assuming the summarized line has format \"start_time stop_time speaker: value\"\n",
    "            parts = line.split(maxsplit=3)  # Split into 4 parts: start_time, stop_time, speaker, and value\n",
    "            if len(parts) == 4:\n",
    "                start_time, stop_time, speaker, value = parts\n",
    "                # Remove any trailing colons or whitespace from the speaker\n",
    "                speaker = speaker.rstrip(':')\n",
    "                writer.writerow([start_time, stop_time, speaker, value])\n",
    "\n",
    "# Sample Snippet\n",
    "# model = OpenAILLM()\n",
    "# model.init(\"gpt-4o-2024-08-06\")\n",
    "# for patient in patients:\n",
    "#   process_transcript(f\"data/DAIC_WOZ/participants/{patient}/{patient}_TRANSCRIPT.csv\", f\"./preprocessed/{patient}_TRANSCRIPT.csv\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb98d547-9d01-4573-9e5c-a745f8ae54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert Truncated Transcripts to JSONL\n",
    "model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "def csv_to_jsonl(csv_file, jsonl_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as csv_f, open(jsonl_file, 'w', encoding='utf-8') as jsonl_f:\n",
    "        reader = csv.DictReader(csv_f, delimiter=',')\n",
    "        # Check the column names for debugging\n",
    "        columns = reader.fieldnames\n",
    "        print(f\"Column names in CSV: {columns}\")  # Debugging step to ensure correct columns\n",
    "        \n",
    "        # Define the system message\n",
    "        system_message = {\"role\": \"system\", \"content\": \"You are a participant in a study where your depression, anxiety and stress is being assessed.\"}\n",
    "        \n",
    "        messages = [system_message]\n",
    "        \n",
    "        for row in reader:\n",
    "            # Determine the role based on the speaker\n",
    "            if row['speaker'].strip().lower() == 'ellie':\n",
    "                role = \"user\"\n",
    "            else:\n",
    "                role = \"assistant\"\n",
    "            \n",
    "            # Append the current message\n",
    "            messages.append({\"role\": role, \"content\": row['value'].strip()})\n",
    "            \n",
    "            # Write to JSONL file after each pair (when role is \"assistant\")\n",
    "            if role == \"assistant\":\n",
    "                jsonl_f.write(json.dumps({\"messages\": messages}) + '\\n')\n",
    "                # Reset messages for the next pair, keeping the system message\n",
    "                messages = [system_message]\n",
    "\n",
    "# Do this Conversion for Each Participant\n",
    "# for patient in patients:\n",
    "#     csv_file = f\"preprocessed/{patient}_TRANSCRIPT.csv\"\n",
    "#     jsonl_file = f\"formatted_final/{patient}_TRANSCRIPT_formatted_data.jsonl\"\n",
    "#     csv_to_jsonl(csv_file=csv_file, jsonl_file=jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f0d5c6-810a-4c75-97d7-dbbdf4ab8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fine-tune for a Given Transcript\n",
    "\n",
    "def fine_tune_model(file_path):\n",
    "    \"\"\"\n",
    "    Function to fine-tune a model using OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the training data file in JSONL format.\n",
    "    \"\"\"\n",
    "    # Load the environment variables from the .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Access the OpenAI API key\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    client = OpenAI()\n",
    "\n",
    "    try:\n",
    "        # Upload the file\n",
    "        print(\"Uploading file...\")\n",
    "        response = files.create(\n",
    "            file=open(file_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "        )\n",
    "\n",
    "        # Retrieve file ID from the uploaded file\n",
    "        file_id = response.id\n",
    "        print(f\"File uploaded successfully. File ID: {file_id}\")\n",
    "\n",
    "        # Start fine-tuning based on the file\n",
    "        print(\"Starting fine-tuning job...\")\n",
    "        ft_response = client.fine_tuning.jobs.create(\n",
    "            training_file=file_id,\n",
    "            model=\"gpt-4o-mini-2024-07-18\"\n",
    "        )\n",
    "\n",
    "        # Fine-tuning job ID\n",
    "        response_id = ft_response.id\n",
    "        print(f\"Fine-tuning job started. Job ID: {response_id}\")\n",
    "\n",
    "        # Training loop - monitor fine-tuning job\n",
    "        while True:\n",
    "            # Retrieve the status of the fine-tuning job\n",
    "            response = client.fine_tuning.jobs.retrieve(response_id)\n",
    "            status = response.status\n",
    "\n",
    "            # Print the current status\n",
    "            print(f\"Fine-tuning job status: {status}\")\n",
    "\n",
    "            # Check if the fine-tuning is completed (either succeeded, failed, or cancelled)\n",
    "            if status in ['succeeded', 'failed', 'cancelled']:\n",
    "                break\n",
    "\n",
    "            # Wait for the specified interval before checking again\n",
    "            time.sleep(120)\n",
    "\n",
    "        # Final status update once the loop ends\n",
    "        if status == 'succeeded':\n",
    "            print(f\"Fine-tuning job completed successfully! Fine-tuned model ID: {response.fine_tuned_model}\")\n",
    "            return response.fine_tuned_model\n",
    "        elif status == 'failed':\n",
    "            print(\"Fine-tuning job failed.\")\n",
    "            return None\n",
    "        elif status == 'cancelled':\n",
    "            print(\"Fine-tuning job was cancelled.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample Usage:\n",
    "# model = fine_tune_model(file_path)\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=model,\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a participant in a psychological study which is measuring your Depression, Anxiety and Stress\"},\n",
    "#     {\"role\": \"user\", \"content\": \"How are you feeling today?\"}\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1ca862-8066-47dd-8b29-0223646ea525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Query the Model (Better Responses using OpenAI Playground due to API suffering from quality degradation due to API\n",
    "# response time constraints)\n",
    "\n",
    "prompts = [\"Little interest or pleasure in doing things.\", \n",
    "           \"Feeling down, depressed, or hopeless\", \n",
    "           \"Trouble falling or staying asleep, or sleeping too much\",\n",
    "           \"Feeling tired or having little energy\",\n",
    "           \"Poor appetite or overeating\",\n",
    "           \"Feeling bad about yourself or that you are a failure or have let yourself or your family down\",\n",
    "           \"Trouble concentrating on things, such as reading the newspaper or watching television\",\n",
    "           \"Moving or speaking so slowly that other people could have noticed. Or the opposite being so fidgety or restless that you have been moving around a lot more than usual\"]\n",
    "\n",
    "# model = fine_tune_model(file_path)\n",
    "\n",
    "def get_phq8_scores(prompts, model):\n",
    "    \"\"\"\n",
    "    Get PHQ-8 scores for a list of prompts by querying the model.\n",
    "\n",
    "    Args:\n",
    "        prompts (list): List of prompts to evaluate.\n",
    "        model (str): Fine-tuned model ID.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping each prompt to its assigned score.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    prompt_text = \"\\n\".join([f\"{i+1}. {prompt}\" for i, prompt in enumerate(prompts)])\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are a participant in a study where we are measuring your Depression, Anxiety and Stress Levels. Be conservative whilst scoring. Only provide numeric answers.\n",
    "    Answer the questions as if you're the \"user\" on the following scale:\\n\n",
    "    0 - Not at all\\n\n",
    "    1 - Several Days\\n\n",
    "    2 - More than half the days\\n\n",
    "    3 - Nearly Every Day\\n\n",
    "    Then add up the score you assign to each prompt, providing a total.\\n\n",
    "    Give me a numerical answer, exclusive to 0, 1, 2, 3.\\n\n",
    "    Analyse the conversation provided and assign it a PHQ-8 Score.\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = f\"On a scale of 0 - 3, rate the following:\\n\\n{prompt_text}\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240635e-1171-489b-b1eb-fa5c16b56f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
